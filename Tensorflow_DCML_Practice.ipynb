{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNfk08HbptcF8yrV/PqmVcf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/izzaauliyairabby/tensorflow/blob/master/Tensorflow_DCML_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors Basic"
      ],
      "metadata": {
        "id": "6xir20dCrZZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tensor adalah struktur data fundamental dalam Machine Learning dan Deep Learning, terutama dalam framework seperti TensorFlow dan PyTorch. Tensor adalah generalized matrix yang dapat memiliki dimensi lebih dari dua. Dengan kata lain, tensor adalah ekstensi dari skalar, vektor, dan matriks ke dimensi yang lebih tinggi.\n",
        "\n",
        "Konsep Dasar Tensor\n",
        "Skalar (0-D Tensor)\n",
        "\n",
        "Nilai tunggal, seperti angka biasa.\n",
        "Contoh: 5, 3.14.\n",
        "Dimensi: 0 (tanpa arah).\n",
        "Vektor (1-D Tensor)\n",
        "\n",
        "Sekumpulan angka dalam satu dimensi.\n",
        "Contoh: [1, 2, 3].\n",
        "Dimensi: 1 (panjang).\n",
        "Matriks (2-D Tensor)\n",
        "\n",
        "Array dua dimensi (baris dan kolom).\n",
        "Contoh:\n",
        "lua\n",
        "Salin kode\n",
        "[[1, 2, 3],\n",
        " [4, 5, 6]]\n",
        "Dimensi: 2 (baris × kolom).\n",
        "Tensor Tinggi Dimensi (n-D Tensor)\n",
        "\n",
        "Ekstensi dari matriks ke lebih dari dua dimensi.\n",
        "Contoh Tensor 3D:\n",
        "lua\n",
        "Salin kode\n",
        "[[[1, 2],\n",
        "  [3, 4]],\n",
        " [[5, 6],\n",
        "  [7, 8]]]\n",
        "Dimensi: 3 atau lebih (bisa memiliki banyak sumbu).\n",
        "Properti Tensor\n",
        "Rank: Jumlah dimensi dari tensor.\n",
        "\n",
        "Contoh:\n",
        "Skalar: Rank 0\n",
        "Vektor: Rank 1\n",
        "Matriks: Rank 2\n",
        "Tensor 3D: Rank 3\n",
        "Shape: Ukuran tensor di setiap dimensinya.\n",
        "\n",
        "Contoh:\n",
        "Vektor [1, 2, 3] → Shape: (3,)\n",
        "Matriks [[1, 2], [3, 4]] → Shape: (2, 2)\n",
        "Tensor 3D → Shape: (2, 2, 2)\n",
        "Data Type: Jenis data yang disimpan dalam tensor (integer, float, dll).\n",
        "\n",
        "Contoh: int32, float32, float64.\n"
      ],
      "metadata": {
        "id": "XS81lQAJkC5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3u-Eo7FqqoL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Initialization of Tensor\n",
        "x = tf.constant(4, shape=(1,1), dtype=tf.float32)\n",
        "x = tf.constant([[1,2,3],[4,5,6]])\n",
        "x = tf.ones((3,3))\n",
        "x = tf.zeros((2,3))\n",
        "x = tf.eye(3) # Identity Matrix\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.normal((3,3), mean=0, stddev=1)\n",
        "x = tf.random.uniform((1,3), minval=0, maxval=1)\n",
        "x = tf.range(9)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "If2wUH4bufbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mathematical Operaions\n",
        "x = tf.constant([1,2,3])\n",
        "y = tf.constant([9,8,7])\n",
        "\n",
        "z = tf.add(x,y)\n",
        "z = x + y\n",
        "\n",
        "z = tf.subtract(x,y)\n",
        "z = x - y\n",
        "\n",
        "z = tf.divide(x,y)\n",
        "z = x / y\n",
        "\n",
        "z = tf.multiply(x,y)\n",
        "z = x * y\n",
        "\n",
        "print(z)"
      ],
      "metadata": {
        "id": "PFO9TPaaujTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexing\n",
        "y = tf.constant([[0,1,2,3,4],[5,6,7,8,9]])\n",
        "print(y[0,:])\n",
        "print(y[1,1])"
      ],
      "metadata": {
        "id": "7kqYb5TmvA76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping\n",
        "y = tf.range(9)\n",
        "y = tf.reshape(y, (3,3))\n",
        "print(y)"
      ],
      "metadata": {
        "id": "lbA5OZD-ydDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.transpose(y, perm=[1,0])\n",
        "print(y)"
      ],
      "metadata": {
        "id": "yk3OD4dlyh1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks and Sequentials API"
      ],
      "metadata": {
        "id": "HQakwJHX32yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "33l-YwRvzPGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "9BbKTrSz4ZEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sequential API\n",
        "Sequential API digunakan untuk membangun model linear stack dari lapisan (layers) yang saling berurutan. Ini cocok untuk model sederhana di mana data mengalir secara sequential dari input ke output.\n",
        "\n",
        "  Ciri-ciri:\n",
        "- Lapisan ditambahkan berurutan (satu per satu).\n",
        "Struktur linear tanpa percabangan (branching) atau penggabungan (merging).\n",
        "Sangat mudah dipahami dan diimplementasikan.\n",
        "Tidak cocok untuk arsitektur kompleks seperti residual networks atau multi-input/multi-output models."
      ],
      "metadata": {
        "id": "yvkjWxUljbwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential API\n",
        "x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=15)\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "iGCdZapp4t15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "xLd_PE7PPZMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Functional API\n",
        "Functional API lebih fleksibel dan digunakan untuk membangun model kompleks seperti arsitektur dengan percabangan (branching), multi-input/multi-output, atau skip connections (seperti pada ResNet).\n",
        "\n",
        "  Ciri-ciri:\n",
        "  Lapisan (layers) diperlakukan sebagai fungsi yang menerima input dan mengembalikan output.\n",
        "  Mendukung arsitektur non-linear (percabangan, penggabungan).\n",
        "  Lebih fleksibel dibandingkan Sequential API.\n",
        "  Cocok untuk model kompleks dan kustom."
      ],
      "metadata": {
        "id": "ngI4LKn8jurb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional API\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the input layer\n",
        "inputs = tf.keras.Input(shape=(784,))\n",
        "\n",
        "# Define the first dense layer\n",
        "dense1 = tf.keras.layers.Dense(512, activation='relu')(inputs)\n",
        "\n",
        "# Define the dropout layer\n",
        "dropout = tf.keras.layers.Dropout(0.2)(dense1)\n",
        "\n",
        "# Define the output layer\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(dropout)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=15)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "C7ZvHcq2PaxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization\n",
        "\n",
        "regularization techniques such as L2 regularization (Ridge Regularization) and Dropout helps prevent overfitting in neural networks by ensuring the model generalizes better to unseen data.\n",
        "\n",
        "1. L2 Regularization (Weight Decay)\n",
        "What is L2 Regularization?\n",
        "L2 regularization penalizes large weights in the model by adding a regularization term to the loss function. This term is proportional to the square of the weights' magnitude.\n",
        "\n",
        "The updated loss function with L2 regularization is:\n",
        "\n",
        "Loss\n",
        "L2\n",
        "=\n",
        "Loss\n",
        "original\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑤\n",
        "𝑖\n",
        "2\n",
        "Loss\n",
        "L2\n",
        "​\n",
        " =Loss\n",
        "original\n",
        "​\n",
        " +λ\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " w\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "𝜆\n",
        "λ: Regularization strength (hyperparameter).\n",
        "𝑤\n",
        "𝑖\n",
        "w\n",
        "i\n",
        "​\n",
        " : Model weights.\n",
        "Why Use L2 Regularization?\n",
        "Reduces overfitting by discouraging large weights.\n",
        "Encourages the model to learn simpler, smaller weights."
      ],
      "metadata": {
        "id": "EB7ZVjgI_I-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: add L2 and Regularization\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# ... (rest of your existing code)\n",
        "\n",
        "# 1. Sequential API with L2 Regularization\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(512, activation='relu', input_shape=(784,), kernel_regularizer=tf.keras.regularizers.l2(0.001)), # Added L2 regularization\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# 2. Functional API with L2 Regularization\n",
        "inputs = tf.keras.Input(shape=(784,))\n",
        "dense1 = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(inputs) # Added L2 regularization\n",
        "dropout = tf.keras.layers.Dropout(0.2)(dense1)\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(dropout)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# ... (rest of your existing code for Functional API)"
      ],
      "metadata": {
        "id": "g4jtYsddWufh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Dropout?\n",
        "Dropout randomly sets a fraction of the input units to zero during training. This prevents the network from relying too much on specific neurons and forces it to generalize.\n",
        "\n",
        "Dropout works by:\n",
        "\n",
        "Temporarily \"dropping\" neurons with a probability\n",
        "𝑝\n",
        "p.\n",
        "Preventing co-adaptation of neurons (i.e., reliance on specific pathways)."
      ],
      "metadata": {
        "id": "mf2NC_y0_QCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: dropout\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# ... (rest of your existing code)\n",
        "\n",
        "# Example of using Dropout in a Sequential model:\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
        "  tf.keras.layers.Dropout(0.2),  # Dropout layer with a rate of 0.2 (20% dropout)\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Example of using Dropout in a Functional API model\n",
        "inputs = tf.keras.Input(shape=(784,))\n",
        "dense1 = tf.keras.layers.Dense(512, activation='relu')(inputs)\n",
        "dropout = tf.keras.layers.Dropout(0.5)(dense1) # Dropout layer with rate 0.5\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(dropout)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "su7-gJcPBvig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNNs, GRUs, LSTMs and Bidirectionality"
      ],
      "metadata": {
        "id": "crY8YHLkB6oD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), Long Short-Term Memory Networks (LSTMs), and Bidirectionality are all essential components in sequence modeling tasks, such as time series forecasting, natural language processing (NLP), and speech recognition."
      ],
      "metadata": {
        "id": "CswAYTxOCDQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "# Example using LSTM in a Sequential model:\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=1000, output_dim=64), # Example embedding layer\n",
        "    tf.keras.layers.LSTM(128), # LSTM layer with 128 units\n",
        "    tf.keras.layers.Dense(10, activation='softmax') # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Example training data (replace with your actual data)\n",
        "import numpy as np\n",
        "x_train = np.random.randint(0, 1000, size=(100, 10)) # Example input sequences\n",
        "y_train = np.random.randint(0, 10, size=(100,))      # Example target labels\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "ruU4NV8GB57T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example using GRU in a Functional API model\n",
        "inputs = tf.keras.Input(shape=(None,))  # Variable-length sequences\n",
        "embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)(inputs) # Example embedding layer\n",
        "gru = tf.keras.layers.GRU(128)(embedding)  # GRU layer with 128 units\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(gru) # Output layer\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Example training data (replace with your actual data)\n",
        "import numpy as np\n",
        "x_train = np.random.randint(0, 1000, size=(100, 10)) # Example input sequences\n",
        "y_train = np.random.randint(0, 10, size=(100,))      # Example target labels\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "KlTcBKzfGeup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Bidirectional LSTM in a Sequential model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=1000, output_dim=64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), # Bidirectional LSTM\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Example training data (replace with your actual data)\n",
        "import numpy as np\n",
        "x_train = np.random.randint(0, 1000, size=(100, 10)) # Example input sequences\n",
        "y_train = np.random.randint(0, 10, size=(100,))      # Example target labels\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "Kz87_8qyHoPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More in Depth Example on Functional API\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Functional API with multiple inputs\n",
        "input_a = tf.keras.Input(shape=(28, 28, 1), name=\"input_a\")\n",
        "input_b = tf.keras.Input(shape=(10,), name=\"input_b\")\n",
        "\n",
        "x = tf.keras.layers.Conv2D(32, 3, activation=\"relu\")(input_a)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.concatenate([x, input_b]) # Concatenating the two inputs\n",
        "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "output = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_a, input_b], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Example input data (replace with your actual data)\n",
        "# Generate dummy data\n",
        "num_samples = 1000\n",
        "\n",
        "x_train_a = np.random.rand(num_samples, 28, 28, 1)\n",
        "x_train_b = np.random.rand(num_samples, 10)\n",
        "y_train = tf.keras.utils.to_categorical(np.random.randint(0, 10, size=(num_samples,)), num_classes=10)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit({\"input_a\": x_train_a, \"input_b\": x_train_b}, y_train, epochs=5)\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "gNt_y7STHo8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model subclassing in Keras allows you to create custom models by subclassing the tf.keras.Model class. This gives you full control over the model's behavior, enabling the use of custom layers, loss functions, and training loops. This approach is often used when the model architecture is more complex than what can be achieved with the functional API or the Sequential API.\n",
        "\n",
        "Here’s a basic guide on how to use model subclassing in Keras:\n",
        "\n",
        "1. Subclassing tf.keras.Model\n",
        "To create a custom model, you subclass tf.keras.Model and implement two key methods:\n",
        "\n",
        "__init__(self): Define the layers and components of the model.\n",
        "call(self, inputs): Define the forward pass (how data flows through the model).\n",
        "Example: Custom MLP Model\n",
        "python\n",
        "Salin kode\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class CustomMLP(tf.keras.Model):\n",
        "    def __init__(self, units=64, num_classes=10):\n",
        "        super(CustomMLP, self).__init__()\n",
        "        # Define layers\n",
        "        self.dense1 = layers.Dense(units, activation='relu')\n",
        "        self.dense2 = layers.Dense(units, activation='relu')\n",
        "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Define forward pass\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Instantiate the model\n",
        "model = CustomMLP()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "2. Training the Model\n",
        "Once the model is defined, you can train it just like any other Keras model.\n",
        "\n",
        "python\n",
        "Salin kode\n",
        "# Generate dummy data\n",
        "import numpy as np\n",
        "x_train = np.random.rand(1000, 784)  # 1000 samples, 784 features\n",
        "y_train = np.random.randint(0, 10, 1000)  # 1000 labels, 10 classes\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "3. Customizing the call Method\n",
        "The call method can be customized to implement more complex architectures, including skip connections, conditional branching, or using custom layers.\n",
        "\n",
        "Example: Custom Model with Skip Connections\n",
        "python\n",
        "Salin kode\n",
        "class SkipConnectionModel(tf.keras.Model):\n",
        "    def __init__(self, units=64, num_classes=10):\n",
        "        super(SkipConnectionModel, self).__init__()\n",
        "        self.dense1 = layers.Dense(units, activation='relu')\n",
        "        self.dense2 = layers.Dense(units, activation='relu')\n",
        "        self.dense3 = layers.Dense(units, activation='relu')\n",
        "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x1 = self.dense1(inputs)\n",
        "        x2 = self.dense2(x1)\n",
        "        x3 = self.dense3(x2)\n",
        "        # Adding skip connection\n",
        "        return self.output_layer(x1 + x3)  # Skip connection from dense1 to output\n",
        "\n",
        "# Instantiate and compile the model\n",
        "model = SkipConnectionModel()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "4. Custom Training Loop (Optional)\n",
        "You can also implement a custom training loop by overriding the train_step and test_step methods. This is useful when you need to have more control over the training process, such as when implementing custom loss functions or optimizers.\n",
        "\n",
        "python\n",
        "Salin kode\n",
        "class CustomTrainingModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(CustomTrainingModel, self).__init__()\n",
        "        self.dense = layers.Dense(10)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compiled_loss(y, y_pred)\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        y_pred = self(x, training=False)\n",
        "        loss = self.compiled_loss(y, y_pred)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "# Instantiate and compile the model\n",
        "model = CustomTrainingModel()\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "Key Takeaways:\n",
        "Subclassing tf.keras.Model provides flexibility to define complex model architectures.\n",
        "You can override the call method to define how data flows through the model.\n",
        "You can implement custom training loops to gain more control over the training process.\n",
        "This approach is particularly useful when you need to implement custom behavior, such as complex layer structures or advanced optimization techniques."
      ],
      "metadata": {
        "id": "QihzQzBdRl4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas\n",
        "\n",
        "if pd == pandas:\n",
        "  print(\"Do Nothing Or Die as a loser\")"
      ],
      "metadata": {
        "id": "ncfqkyK5RCLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Layers\n",
        "\n",
        "Custom Layers in TensorFlow are user-defined neural network layers that extend the functionality of the built-in layers. They are particularly useful when you need a layer with behavior that isn’t available in TensorFlow’s standard library. By creating a custom layer, you can define unique computations, parameters, and forward passes tailored to your specific requirements.\n",
        "\n",
        "Why Use Custom Layers?\n",
        "Specialized Computations: When standard layers like Dense, Conv2D, etc., don’t meet your needs.\n",
        "Reusability: Custom layers can encapsulate complex logic and be reused across different models.\n",
        "Flexibility: Allows fine-grained control over computations and parameters.\n",
        "How to Create a Custom Layer\n",
        "Custom layers are implemented by subclassing tf.keras.layers.Layer and overriding the following methods:\n",
        "\n",
        "__init__: Initialize the layer and its parameters.\n",
        "build(self, input_shape): Define the layer’s weights and perform initialization.\n",
        "call(self, inputs): Specify the forward computation.\n",
        "Example 1: A Simple Custom Layer\n",
        "This layer multiplies the input by a constant factor.\n"
      ],
      "metadata": {
        "id": "nKDxnnwnh7rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: custom layers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class CustomDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units=32, activation=None):\n",
        "    super(CustomDenseLayer, self).__init__()\n",
        "    self.units = units\n",
        "    self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(\n",
        "        shape=(input_shape[-1], self.units),\n",
        "        initializer='random_normal',\n",
        "        trainable=True\n",
        "    )\n",
        "    self.b = self.add_weight(\n",
        "        shape=(self.units,),\n",
        "        initializer='random_normal',\n",
        "        trainable=True\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.activation(tf.matmul(inputs, self.w) + self.b)\n",
        "\n",
        "# Example usage in a Sequential model:\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    CustomDenseLayer(32, activation='sigmoid'),  # Use the custom layer\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Example usage in a Functional API model:\n",
        "inputs = tf.keras.Input(shape=(784,))\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
        "x = CustomDenseLayer(32, activation='sigmoid')(x) # Use the custom layer\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "Fnpsg264SPGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Savings and Load Models\n",
        "In TensorFlow, saving and loading models is essential for deploying them, fine-tuning, or resuming training. TensorFlow provides two main formats for saving models:\n",
        "\n",
        "Checkpoint Format: Saves the model's weights (parameters) only. Useful for resuming training.\n",
        "SavedModel Format: Saves the entire model, including architecture, weights, and optimizer configuration. Useful for deploying or sharing models."
      ],
      "metadata": {
        "id": "QSSvCY8ia13s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rvIZSh1txr5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and load model\n",
        "\n",
        "# Save the model\n",
        "model.save('my_model')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model('my_model')\n",
        "\n",
        "# Verify that the loaded model is the same as the original model\n",
        "# (Optional) You can further evaluate the loaded model to ensure accuracy.\n",
        "loaded_model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "Srz8uLr2PN6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "Definition: Transfer learning involves using a pre-trained model as a starting point for a new task. Typically, the pre-trained model's weights are frozen, and only the new layers added for the specific task are trained.\n",
        "\n",
        "Steps for Transfer Learning\n",
        "Load a pre-trained model.\n",
        "Freeze the pre-trained layers.\n",
        "Add new layers for the target task.\n",
        "Train only the new layers.\n"
      ],
      "metadata": {
        "id": "RRr5JLUtCCvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: transfer learning\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load a pre-trained model (e.g., MobileNetV2)\n",
        "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model's layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add new layers for your task (e.g., image classification)\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = base_model(inputs, training=False)  # Ensure the base model is in inference mode\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(1024, activation='relu')(x) # Example dense layer\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(x) # Example output layer for 10 classes\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Generate dummy data for demonstration\n",
        "num_samples = 100\n",
        "x_train = np.random.rand(num_samples, 224, 224, 3)\n",
        "y_train = tf.keras.utils.to_categorical(np.random.randint(0, 10, size=(num_samples,)), num_classes=10)\n",
        "\n",
        "# Train the model (only the new layers will be trained)\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "DeQbcB4x3CxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning\n",
        "\n",
        "Definition: Fine-tuning involves unfreezing some or all layers of the pre-trained model and retraining them along with the new layers. This allows the model to adapt more specifically to the new task.\n",
        "\n",
        "Steps for Fine-Tuning\n",
        "Perform transfer learning as above.\n",
        "Unfreeze some or all layers of the pre-trained model.\n",
        "Use a lower learning rate to avoid destroying pre-trained weights.\n",
        "Train the model.\n"
      ],
      "metadata": {
        "id": "OedJHbUuCktb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: fine tuning\n",
        "\n",
        "# Unfreeze some layers of the base model (e.g., the top layers)\n",
        "base_model.trainable = True\n",
        "fine_tune_at = 100  # Unfreeze layers from this point onwards\n",
        "\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  # Lower learning rate\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Continue training the model\n",
        "model.fit(x_train, y_train, epochs=10) # Continue training with a lower learning rate\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "lTmOot9GClr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# TensorFlow Hub\n",
        "TensorFlow Hub is a library and repository for reusable machine learning modules. You can use models from TensorFlow Hub for tasks like image classification, text embedding, and more.\n",
        "\n",
        "Finding Models on TensorFlow Hub\n",
        "Browse models at TensorFlow Hub."
      ],
      "metadata": {
        "id": "ggF1ZELSCmGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorflow hub\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Load a pre-trained model from TensorFlow Hub\n",
        "model = hub.load(\"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/classification/5\") # Example\n",
        "\n",
        "# Example usage (assuming you have image data)\n",
        "# image = ... # Load your image data\n",
        "\n",
        "# Make predictions\n",
        "# predictions = model(image)"
      ],
      "metadata": {
        "id": "8ivYrfwRCnQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow Datasets\n",
        "Pre built Datasets"
      ],
      "metadata": {
        "id": "DoEu3gdhNaWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorflow datasets\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load a dataset (e.g., MNIST)\n",
        "dataset, info = tfds.load('mnist', with_info=True, as_supervised=True)\n",
        "\n",
        "# Access the training and testing splits\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# Print dataset information\n",
        "print(info)\n",
        "\n",
        "# Example: Iterate through the training data and print the first 10 images and labels\n",
        "for image, label in train_dataset.take(10):\n",
        "  print(f\"Image shape: {image.shape}, Label: {label}\")"
      ],
      "metadata": {
        "id": "ctg3RzpQNYnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "SX5EwlgXOq_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation is a technique used in machine learning to artificially expand the size of a training dataset by applying transformations to the original data. It is commonly used in image, text, and audio datasets to improve model performance and generalization, especially when the dataset is small or imbalanced.\n",
        "\n",
        "Why Use Data Augmentation?\n",
        "Increase Dataset Size: Generates more training examples from limited data.\n",
        "Reduce Overfitting: Makes the model more robust by introducing variations.\n",
        "Improve Generalization: Helps the model perform better on unseen data.\n",
        "Handle Imbalanced Datasets: Balances class distribution by augmenting minority class samples.\n",
        "Types of Data Augmentation\n",
        "1. Image Data Augmentation\n",
        "Transforms images to simulate real-world variations.\n",
        "\n",
        "Geometric Transformations:\n",
        "Flipping (horizontal/vertical)\n",
        "Rotation\n",
        "Cropping\n",
        "Scaling\n",
        "Translation (shifting)\n",
        "Color Transformations:\n",
        "Brightness adjustment\n",
        "Contrast adjustment\n",
        "Hue and saturation changes\n",
        "Noise Addition:\n",
        "Gaussian noise\n",
        "Salt-and-pepper noise\n",
        "Advanced Techniques:\n",
        "CutMix\n",
        "MixUp\n",
        "Random Erasing"
      ],
      "metadata": {
        "id": "eOCcLFchQu-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Example data augmentation using tf.image\n",
        "def augment_image(image, label):\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "  image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "  return image, label\n",
        "\n",
        "# Apply augmentation to the dataset\n",
        "augmented_train_dataset = train_dataset.map(augment_image)"
      ],
      "metadata": {
        "id": "TCP_YENNNh7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callback and Custom Callback"
      ],
      "metadata": {
        "id": "1mp1ADX6OzAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback and Custom Callback\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define a custom callback\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f\"Finished epoch {epoch}, loss: {logs['loss']:.4f}, accuracy: {logs['accuracy']:.4f}\")\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        print(f\"Finished batch {batch}, loss: {logs['loss']:.4f}\")\n",
        "\n",
        "# Example usage\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,))\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Generate dummy data\n",
        "x_train = tf.random.normal((100, 784))\n",
        "y_train = tf.random.uniform((100,), minval=0, maxval=10, dtype=tf.int32)\n",
        "\n",
        "# Train the model with the custom callback\n",
        "model.fit(x_train, y_train, epochs=2, callbacks=[CustomCallback()])"
      ],
      "metadata": {
        "id": "aQGRei6uOyHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customizing Model Fit"
      ],
      "metadata": {
        "id": "c1tXzM4VPF1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Model Fit\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_fit(model, x_train, y_train, epochs=10, learning_rate=0.001):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        for batch in range(len(x_train) // 32):  # Example batch size of 32\n",
        "            with tf.GradientTape() as tape:\n",
        "                batch_x = x_train[batch * 32:(batch + 1) * 32]\n",
        "                batch_y = y_train[batch * 32:(batch + 1) * 32]\n",
        "                predictions = model(batch_x)\n",
        "                loss = loss_fn(batch_y, predictions)\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            # Print loss for every 10 batches\n",
        "            if batch % 10 == 0:\n",
        "                print(f\"Batch {batch}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Example usage (replace with your actual model and data)\n",
        "# Assume you have defined 'model', 'x_train', and 'y_train'\n",
        "# Create dummy data and model for demonstration\n",
        "x_train = tf.random.normal((100, 784))\n",
        "y_train = tf.random.uniform((100,), minval=0, maxval=10, dtype=tf.int32)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='softmax', input_shape=(784,))\n",
        "])\n",
        "custom_fit(model, x_train, y_train, epochs=2)"
      ],
      "metadata": {
        "id": "IdhxFmAaPLM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Training Loops"
      ],
      "metadata": {
        "id": "6Mx7n5BbQUPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: cusom training loops\n",
        "\n",
        "# Example of a custom training loop with gradient accumulation\n",
        "def custom_fit_with_gradient_accumulation(model, x_train, y_train, epochs=10, learning_rate=0.001, accumulation_steps=4):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables]  # Initialize accumulated gradients\n",
        "\n",
        "        for batch in range(len(x_train) // 32):\n",
        "            with tf.GradientTape() as tape:\n",
        "                batch_x = x_train[batch * 32:(batch + 1) * 32]\n",
        "                batch_y = y_train[batch * 32:(batch + 1) * 32]\n",
        "                predictions = model(batch_x)\n",
        "                loss = loss_fn(batch_y, predictions)\n",
        "\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            accumulated_gradients = [a + g for a, g in zip(accumulated_gradients, gradients)] # Accumulate gradients\n",
        "\n",
        "            if (batch + 1) % accumulation_steps == 0:  # Apply gradients after accumulation_steps\n",
        "                optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
        "                accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables] # Reset accumulated gradients\n",
        "                print(f\"Batch {batch + 1}, Loss: {loss.numpy()}\")"
      ],
      "metadata": {
        "id": "klku2Dy3PSdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete TensorBoard Guide"
      ],
      "metadata": {
        "id": "WYMM9yPzQ6Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Complete TensorBoard Guide\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/\n",
        "\n",
        "# Define the TensorBoard callback\n",
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# ... (Your model definition and data loading code here) ...\n",
        "\n",
        "# Train your model with the TensorBoard callback\n",
        "model.fit(x_train, y_train, epochs=10, callbacks=[tensorboard_callback])\n",
        "\n",
        "# Start TensorBoard\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "Oqe5zjsVQ4zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Dataset for Images"
      ],
      "metadata": {
        "id": "zinhD9fTRTIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Custom Dataset for Images\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Define the path to your image dataset directory\n",
        "image_dir = \"/path/to/your/image/dataset\"  # Replace with the actual path\n",
        "\n",
        "# Create a list to store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through the subdirectories (classes) in the image directory\n",
        "for class_name in os.listdir(image_dir):\n",
        "    class_dir = os.path.join(image_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for image_name in os.listdir(class_dir):\n",
        "            image_path = os.path.join(class_dir, image_name)\n",
        "            image_paths.append(image_path)\n",
        "            labels.append(class_name)\n",
        "\n",
        "# Convert labels to numerical values\n",
        "unique_labels = sorted(list(set(labels)))\n",
        "label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
        "numerical_labels = [label_to_index[label] for label in labels]\n",
        "\n",
        "# Create a TensorFlow dataset from image paths and labels\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices((image_paths, numerical_labels))\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(image_path, label):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)  # Adjust channels if needed\n",
        "    image = tf.image.resize(image, [224, 224])  # Resize to desired size\n",
        "    image = tf.cast(image, tf.float32) / 255.0 # Normalize pixel values\n",
        "    return image, label\n",
        "\n",
        "# Apply the function to the dataset\n",
        "image_dataset = image_dataset.map(load_and_preprocess_image)\n",
        "\n",
        "# Further transformations (e.g., shuffling, batching)\n",
        "image_dataset = image_dataset.shuffle(buffer_size=len(image_paths))\n",
        "image_dataset = image_dataset.batch(32) # Example batch size\n",
        "\n",
        "\n",
        "# Example usage in model training:\n",
        "# model.fit(image_dataset, epochs=10)"
      ],
      "metadata": {
        "id": "0QrnVW5ERMNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Dataset for Text with TextLineDataset"
      ],
      "metadata": {
        "id": "kqqFac8PRX2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Custom Dataset for Text with TextLineDataset\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Example text file (replace with your actual file)\n",
        "text_file_path = \"text_file.txt\"  # Replace with the path to your text file\n",
        "\n",
        "# Create a TextLineDataset\n",
        "text_dataset = tf.data.TextLineDataset(text_file_path)\n",
        "\n",
        "\n",
        "# Function to preprocess text lines\n",
        "def preprocess_text(line):\n",
        "    # Example preprocessing: lowercase, remove whitespace\n",
        "    line = tf.strings.lower(line)\n",
        "    line = tf.strings.strip(line)\n",
        "    # Add more preprocessing steps as needed (e.g., tokenization, filtering)\n",
        "    return line\n",
        "\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "preprocessed_dataset = text_dataset.map(preprocess_text)\n",
        "\n",
        "\n",
        "# Example usage in a model\n",
        "# Assuming you have defined a model (e.g., using TensorFlow Hub or custom layers)\n",
        "\n",
        "# Batch and shuffle the dataset\n",
        "BATCH_SIZE = 32  # Adjust as needed\n",
        "BUFFER_SIZE = 10000 # Adjust as needed\n",
        "preprocessed_dataset = preprocessed_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# Iterate over batches in the dataset and train the model\n",
        "# Replace with your actual model and training loop\n",
        "for text_batch in preprocessed_dataset:\n",
        "    # Process and train the batch here\n",
        "text_batch"
      ],
      "metadata": {
        "id": "qrMEN4pkRecB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying Skin Cancer"
      ],
      "metadata": {
        "id": "-_u09LAFRksF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Classifying Skin Cancer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the HAM10000 dataset (replace with your actual dataset path if needed)\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'ham10000',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)  # Convert to float32\n",
        "    image = tf.image.resize(image, size=(224, 224))  # Resize images to 224x224\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# Apply preprocessing to the datasets\n",
        "ds_train = ds_train.map(preprocess).cache().shuffle(ds_info.splits['train'].num_examples).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.map(preprocess).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create the model (example: a simple CNN)\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(7, activation='softmax')  # 7 output classes (skin cancer types)\n",
        "])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(ds_train, epochs=10, validation_data=ds_test) # Reduced epochs for demonstration\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(ds_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5FeVCN-MRhAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}