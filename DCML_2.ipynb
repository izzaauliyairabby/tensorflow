{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyP/v+93Qu0A3A+auNgHsGVw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/izzaauliyairabby/tensorflow/blob/master/DCML_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Installation\n",
        "\n",
        "Install TensorFlow 2.13.x: understand how to install TensorFlow and the required dependencies.\n",
        "Setup Development Environment: setting up the development environment, including an IDE or Jupyter notebook."
      ],
      "metadata": {
        "id": "_tv7mUmIXl74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install TensorFlow 2\n",
        "TensorFlow is tested and supported on the following 64-bit systems:\n",
        "\n",
        "Python 3.8–3.11\n",
        "Ubuntu 16.04 or later\n",
        "Windows 7 or later (with C++ redistributable)\n",
        "macOS 10.12.6 (Sierra) or later (no GPU support)\n",
        "WSL2 via Windows 10 19044 or higher including GPUs (Experimental)\n",
        "\n",
        "System requirements\n",
        "Ubuntu 16.04 or higher (64-bit)\n",
        "macOS 12.0 (Monterey) or higher (64-bit) (no GPU support)\n",
        "Windows Native - Windows 7 or higher (64-bit) (no GPU support after TF 2.10)\n",
        "Windows WSL2 - Windows 10 19044 or higher (64-bit)\n",
        "Note: GPU support is available for Ubuntu and Windows with CUDA®-enabled cards.\n",
        "Software requirements\n",
        "Python 3.9–3.12\n",
        "pip version 19.0 or higher for Linux (requires manylinux2014 support) and Windows. pip version 20.3 or higher for macOS.\n",
        "Windows Native Requires Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019\n",
        "The following NVIDIA® software are only required for GPU support.\n",
        "\n",
        "NVIDIA® GPU drivers\n",
        ">= 525.60.13 for Linux\n",
        ">= 528.33 for WSL on Windows\n",
        "CUDA® Toolkit 12.3.\n",
        "cuDNN SDK 8.9.7.\n",
        "(Optional) TensorRT to improve latency and throughput for inference."
      ],
      "metadata": {
        "id": "jc8D88kTcg1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mx0OOWYSWjeD"
      },
      "outputs": [],
      "source": [
        "# Requires the latest pip\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Current stable release for CPU and GPU\n",
        "!pip install tensorflow\n",
        "\n",
        "# Or try the preview build (unstable)\n",
        "!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# USING DOCKERS\n",
        "#  docker pull tensorflow/tensorflow:latest  # Download latest stable image\n",
        "#  docker run -it -p 8888:8888 tensorflow/tensorflow:latest-jupyter  # Start Jupyter server"
      ],
      "metadata": {
        "id": "n_30iY6XcpSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Data Pipeline\n",
        "\n",
        "- Data Loading and Preprocessing: using tf.data to load and process the dataset.\n",
        "- Data Augmentation: applying data augmentation techniques to enlarge the dataset and improve the generalization ability of the model.\n",
        "- Normalization and Standardization: normalize and standardize the data before using it in the model."
      ],
      "metadata": {
        "id": "w5XOtkw5e4us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data Loading and Preprocessing\n",
        "Tujuan: Memuat dan memproses dataset menggunakan tf.data.\n",
        "\n",
        "Langkah-langkah:\n",
        "- Memuat Dataset\n",
        "- Dataset bisa berasal dari array, file CSV, atau dataset bawaan TensorFlow.\n",
        "\n",
        "Transformasi Dataset\n",
        "\n",
        "- Shuffle: Mengacak data untuk menghindari bias urutan.\n",
        "- Batch: Membagi data menjadi batch untuk efisiensi pelatihan.\n",
        "- Prefetch: Memuat data paralel untuk mempercepat pelatihan."
      ],
      "metadata": {
        "id": "88NW5tnweD6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA LOADING AND PREPROCESSING\n",
        "import tensorflow as tf\n",
        "\n",
        "# Memuat dataset MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalisasi awal (dari 0-255 ke 0-1)\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Membuat dataset dengan tf.data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "zLgT4_6Veu9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Augmentation\n",
        "Tujuan: Memperbesar dataset dengan memodifikasi data secara acak untuk meningkatkan kemampuan generalisasi model.\n",
        "\n",
        "Teknik Umum:\n",
        "- Flip: Membalik gambar secara horizontal atau vertikal.\n",
        "- Rotate: Memutar gambar dengan sudut tertentu.\n",
        "- Zoom: Memperbesar atau memperkecil gambar.\n",
        "- Brightness: Mengubah kecerahan gambar."
      ],
      "metadata": {
        "id": "yv7_6xF0eIgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA AUGMENTATION\n",
        "import tensorflow as tf\n",
        "\n",
        "# Fungsi augmentasi\n",
        "def augment(image, label):\n",
        "    \"\"\"Applies random flip and brightness adjustments to the image.\n",
        "\n",
        "    Args:\n",
        "        image: The input image tensor.\n",
        "        label: The corresponding label for the image.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the augmented image and label.\n",
        "    \"\"\"\n",
        "    image = tf.expand_dims(image, axis=-1)  # Add channel dimension if it doesn't exist\n",
        "    image = tf.image.random_flip_left_right(image)  # Randomly flip the image horizontally\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)  # Adjust image brightness\n",
        "    return image, label\n",
        "\n",
        "# Terapkan augmentasi pada dataset\n",
        "augmented_train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Iterasi untuk melihat hasil augmentasi\n",
        "for images, labels in augmented_train_dataset.take(1):\n",
        "    print(f\"Augmented image shape: {images.shape}\")"
      ],
      "metadata": {
        "id": "yhj-gYjxd1Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Normalization and Standardization\n",
        "Tujuan:\n",
        "\n",
        "- Normalisasi: Mengubah nilai piksel menjadi rentang tertentu (misalnya 0-1).\n",
        "- Standardisasi: Mengubah nilai data sehingga memiliki rata-rata 0 dan standar deviasi 1.\n",
        "- Normalisasi:\n",
        "Menggunakan nilai minimum dan maksimum dari dataset."
      ],
      "metadata": {
        "id": "Rf-vAHyXeZYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi (0-255 ke 0-1)\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "AB9EDj0regmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardisasi:\n",
        "Menggunakan rata-rata dan standar deviasi dataset."
      ],
      "metadata": {
        "id": "CUVYWd4KejrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardisasi\n",
        "mean = train_images.mean()\n",
        "std = train_images.std()\n",
        "\n",
        "train_images = (train_images - mean) / std\n",
        "test_images = (test_images - mean) / std"
      ],
      "metadata": {
        "id": "OMVYXKlmer1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mengintegrasikan Normalisasi dan Standardisasi dalam Pipeline:"
      ],
      "metadata": {
        "id": "LwqfYo1RewaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_standardize(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Normalisasi\n",
        "    image = (image - tf.reduce_mean(image)) / tf.math.reduce_std(image)  # Standardisasi\n",
        "    return image, label\n",
        "\n",
        "# Terapkan pada dataset\n",
        "normalized_train_dataset = train_dataset.map(normalize_and_standardize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "normalized_test_dataset = test_dataset.map(normalize_and_standardize, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "ojWcRtc9e3jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline Lengkap"
      ],
      "metadata": {
        "id": "fppIUMvrfQ3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Memuat dataset MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalisasi awal\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Membuat dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "# Fungsi augmentasi\n",
        "def augment(image, label):\n",
        "    image = tf.expand_dims(image, axis = 1)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    return image, label\n",
        "\n",
        "# Fungsi normalisasi dan standardisasi\n",
        "def normalize_and_standardize(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image - tf.reduce_mean(image)) / tf.math.reduce_std(image)\n",
        "    return image, label\n",
        "\n",
        "# Pipeline lengkap\n",
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .shuffle(10000)\n",
        "    .map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .map(normalize_and_standardize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(32)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    test_dataset\n",
        "    .map(normalize_and_standardize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(32)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Iterasi untuk melihat hasil akhir\n",
        "for images, labels in train_dataset.take(1):\n",
        "    print(f\"Final image shape: {images.shape}\")\n",
        "    print(f\"Final label shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "qi3nQJpffUkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. TensorFlow Core Concepts\n",
        "\n",
        "- Tensor Operations: understand basic operations on Tensors, such as - - arithmetic calculations, indexing, and reshaping.\n",
        "- Gradient Descent and Backpropagation: understand the basic concepts of optimization using gradient descent and backpropagation algorithms."
      ],
      "metadata": {
        "id": "xwy6W85JiSw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensors Operations"
      ],
      "metadata": {
        "id": "FpEtAZdBiZL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor adalah struktur data utama dalam TensorFlow, mirip dengan array multidimensi.\n",
        "\n",
        "- Operasi Dasar pada Tensor\n",
        "- Arithmetic Calculations: Operasi matematika seperti penjumlahan, pengurangan, perkalian, dan pembagian.\n",
        "- Indexing: Mengakses elemen tensor menggunakan indeks.\n",
        "- Reshaping: Mengubah bentuk tensor tanpa mengubah data."
      ],
      "metadata": {
        "id": "WckAp-4migg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Membuat tensor\n",
        "tensor_a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "tensor_b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
        "\n",
        "# 1. Operasi Matematika\n",
        "add_result = tf.add(tensor_a, tensor_b)  # Penjumlahan\n",
        "mul_result = tf.multiply(tensor_a, tensor_b)  # Perkalian elemen\n",
        "matmul_result = tf.matmul(tensor_a, tensor_b)  # Perkalian matriks\n",
        "\n",
        "print(\"Penjumlahan:\\n\", add_result)\n",
        "print(\"Perkalian Elemen:\\n\", mul_result)\n",
        "print(\"Perkalian Matriks:\\n\", matmul_result)\n",
        "\n",
        "# 2. Indexing\n",
        "print(\"Elemen pertama dari tensor_a:\", tensor_a[0, 0])\n",
        "\n",
        "# 3. Reshaping\n",
        "reshaped_tensor = tf.reshape(tensor_a, [1, 4])  # Ubah bentuk menjadi 1x4\n",
        "print(\"Tensor setelah reshaping:\\n\", reshaped_tensor)\n"
      ],
      "metadata": {
        "id": "NJum0reLgVrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Gradient Descent and Backpropagation\n",
        "Gradient Descent adalah algoritma optimasi untuk meminimalkan fungsi loss.\n",
        "Backpropagation adalah proses menghitung gradien untuk memperbarui bobot model.\n",
        "\n",
        "Konsep Utama:\n",
        "- Loss Function: Mengukur kesalahan antara prediksi dan target.\n",
        "- Gradien: Menghitung turunan fungsi loss terhadap parameter model.\n",
        "- Optimizer: Menggunakan gradien untuk memperbarui parameter model.\n",
        " Contoh Kode Gradient Descent:"
      ],
      "metadata": {
        "id": "dgFSGT1cjJU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi Loss (Mean Squared Error)\n",
        "def loss_fn(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "# Model Sederhana\n",
        "w = tf.Variable(2.0)  # Inisialisasi bobot\n",
        "x = tf.constant([1.0, 2.0, 3.0, 4.0])  # Input\n",
        "y_true = tf.constant([2.0, 4.0, 6.0, 8.0])  # Target\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 0.01\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Gradient Descent Loop\n",
        "for step in range(100):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = w * x  # Prediksi\n",
        "        loss = loss_fn(y_true, y_pred)  # Hitung loss\n",
        "\n",
        "    # Hitung gradien\n",
        "    gradients = tape.gradient(loss, [w])\n",
        "\n",
        "    # Update parameter\n",
        "    optimizer.apply_gradients(zip(gradients, [w]))\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step}, Loss: {loss.numpy()}, Weight: {w.numpy()}\")\n",
        "\n",
        "# GradientTape: Merekam operasi untuk menghitung gradien.\n",
        "# Optimizer: SGD digunakan untuk memperbarui bobot model.\n",
        "# apply_gradients: Mengaplikasikan gradien ke parameter model."
      ],
      "metadata": {
        "id": "6GSVE2TTjOzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contoh Backpropagation pada Neural Network"
      ],
      "metadata": {
        "id": "PkM1yBUgjlaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Sederhana\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, input_shape=(1,))\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "# Data\n",
        "x_train = tf.constant([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_train = tf.constant([[2.0], [4.0], [6.0], [8.0]])\n",
        "\n",
        "# Training\n",
        "model.fit(x_train, y_train, epochs=200, verbose=1)\n",
        "\n",
        "# Layer Dense: Layer linear dengan bobot yang dioptimalkan.\n",
        "# SGD Optimizer: Menggunakan Stochastic Gradient Descent.\n",
        "# fit: Melatih model dengan backpropagation."
      ],
      "metadata": {
        "id": "QtiPofQijs20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Construction"
      ],
      "metadata": {
        "id": "XPnWDPGKjvRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential API\n",
        " digunakan untuk membuat model neural network sederhana dengan arsitektur berlapis secara berurutan."
      ],
      "metadata": {
        "id": "RWUtn8nDmfMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Membuat model Sequential\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),  # Input layer\n",
        "    tf.keras.layers.Dense(32, activation='relu'),  # Hidden layer\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Tampilkan arsitektur model\n",
        "model.summary()\n",
        "\n",
        "# Dense Layer: Layer fully connected.\n",
        "# Activation Function: Menggunakan relu untuk hidden layer dan sigmoid untuk output.\n",
        "# Compile: Mengatur optimizer, fungsi loss, dan metrik evaluasi.\n"
      ],
      "metadata": {
        "id": "NOS5EyFomkRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Functional API\n",
        "Functional API digunakan untuk membangun model yang lebih kompleks, seperti model dengan input/output ganda atau arsitektur bercabang."
      ],
      "metadata": {
        "id": "lNhD3yD3msoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDenseLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(CustomDenseLayer, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "# Menggunakan Custom Layer\n",
        "input_layer = tf.keras.Input(shape=(10,))\n",
        "custom_layer = CustomDenseLayer(64)(input_layer)\n",
        "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(custom_layer)\n",
        "\n",
        "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Jkacj1RJmrto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh Custom Model:\n",
        "class CustomModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Membuat dan melatih model\n",
        "model = CustomModel()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Dummy data untuk latihan\n",
        "import numpy as np\n",
        "x_train = np.random.random((100, 10))\n",
        "y_train = np.random.randint(2, size=(100, 1))\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10)"
      ],
      "metadata": {
        "id": "O7Xq8c89G2bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convlutional Neural Networks\n",
        "\n",
        "- Basic CNNs: build and train CNN models for image classification.\n",
        "- Pooling Layers: using pooling layers such as MaxPooling2D and AveragePooling2D to reduce data dimensionality.\n",
        "- Batch Normalization and Dropout: apply batch normalization and dropout to improve model stability and performance.\n",
        "\n",
        "### 1. Basic CNNs: Membuat dan Melatih Model CNN untuk Klasifikasi Gambar\n",
        "CNN adalah jenis jaringan saraf yang digunakan untuk tugas-tugas seperti klasifikasi gambar."
      ],
      "metadata": {
        "id": "ZQx9QXHTJVSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Membuat model CNN\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),  # Convolutional layer\n",
        "    layers.MaxPooling2D((2, 2)),  # Max pooling\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),  # Convolutional layer\n",
        "    layers.MaxPooling2D((2, 2)),  # Max pooling\n",
        "    layers.Flatten(),  # Flatten layer\n",
        "    layers.Dense(64, activation='relu'),  # Fully connected layer\n",
        "    layers.Dense(10, activation='softmax')  # Output layer (10 kelas)\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Tampilkan arsitektur model\n",
        "model.summary()\n",
        "\n",
        "# Dummy data untuk pelatihan\n",
        "import numpy as np\n",
        "x_train = np.random.random((100, 64, 64, 3))  # 100 gambar berukuran 64x64x3\n",
        "y_train = np.random.randint(10, size=(100,))  # Label dengan 10 kelas\n",
        "\n",
        "# Melatih model\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=16)\n"
      ],
      "metadata": {
        "id": "ad9FYacTJk5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan:\n",
        "- Conv2D: Layer konvolusi untuk mengekstrak fitur dari gambar.\n",
        "- MaxPooling2D: Mengurangi dimensi data dengan pooling maksimum.\n",
        "- Flatten: Mengubah data 2D menjadi 1D untuk input ke fully connected layer.\n",
        "- Dense: Layer fully connected untuk klasifikasi."
      ],
      "metadata": {
        "id": "PHbP-l9KLn28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Pooling Layers\n",
        "Pooling layer digunakan untuk mengurangi dimensi data, mempercepat komputasi, dan mencegah overfitting.\n",
        "\n",
        "Jenis Pooling:\n",
        "MaxPooling2D: Mengambil nilai maksimum dari setiap area pooling.\n",
        "AveragePooling2D: Mengambil rata-rata nilai dari setiap area pooling.\n",
        "\n",
        "Penjelasan:\n",
        "- MaxPooling2D digunakan untuk mengambil fitur dominan.\n",
        "- AveragePooling2D digunakan untuk mengambil rata-rata fitur."
      ],
      "metadata": {
        "id": "eWy1oLJ_L4e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),  # Max pooling\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.AveragePooling2D((2, 2)),  # Average pooling\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "5JSGEbRRLELR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Batch Normalization and Dropout\n",
        "Batch Normalization dan Dropout digunakan untuk meningkatkan stabilitas dan performa model.\n",
        "\n",
        "Batch Normalization:\n",
        "Menormalkan output dari layer sebelumnya, mempercepat pelatihan, dan mengurangi sensitivitas terhadap inisialisasi.\n",
        "\n",
        "Dropout:\n",
        "Mengabaikan beberapa neuron secara acak selama pelatihan untuk mencegah overfitting.\n",
        "\n",
        "Contoh Kode:"
      ],
      "metadata": {
        "id": "trnexbJ_MJuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.BatchNormalization(),  # Batch normalization\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),  # Batch normalization\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Dropout\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "awUtL3SdMDQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan:\n",
        "- Batch Normalization:\n",
        "- Menormalkan aktivasi untuk setiap batch.\n",
        "- Membantu stabilitas pelatihan.\n",
        "Dropout:\n",
        "- Parameter 0.5 berarti 50% neuron akan di-drop selama pelatihan.\n",
        "- Mencegah overfitting dengan membuat model lebih general."
      ],
      "metadata": {
        "id": "3GRM_ftBMUSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Natural Language Processing (NLP)\n",
        "\n",
        "- Text Vectorization: using text tokenization and vectorization techniques, such as the TextVectorization layer.\n",
        "- Word Embeddings: understanding and using word embeddings, such as the Embedding layer.\n",
        "- Recurrent Neural Networks (RNNs): building RNN models, including LSTM and GRU, for text processing."
      ],
      "metadata": {
        "id": "fu1SUgE7MhgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Text Vectorization\n",
        "Text vectorization mengubah teks menjadi representasi numerik yang dapat digunakan oleh model machine learning.\n",
        "\n",
        "TextVectorization Layer:\n",
        "Layer ini digunakan untuk tokenisasi dan vektorisasi teks."
      ],
      "metadata": {
        "id": "Xy4WO4HjSNf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Data teks\n",
        "texts = [\n",
        "    \"I love machine learning\",\n",
        "    \"Natural language processing is amazing\",\n",
        "    \"TensorFlow makes it easy\"\n",
        "]\n",
        "\n",
        "# Membuat TextVectorization layer\n",
        "vectorizer = TextVectorization(max_tokens=1000, output_sequence_length=10)\n",
        "\n",
        "# Menyesuaikan layer dengan data teks\n",
        "vectorizer.adapt(texts)\n",
        "\n",
        "# Melihat hasil vektorisasi\n",
        "vectorized_texts = vectorizer(texts)\n",
        "print(vectorized_texts.numpy())\n"
      ],
      "metadata": {
        "id": "zu0K0ooBMg8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan:\n",
        "- max_tokens: Jumlah maksimum kata unik dalam vektor.\n",
        "- output_sequence_length: Panjang maksimum setiap sequence (padding jika lebih pendek)."
      ],
      "metadata": {
        "id": "W7YxDWxxSVOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Word Embeddings\n",
        "Word embeddings adalah representasi vektor dari kata-kata dalam ruang berdimensi rendah, menangkap hubungan semantik antar kata."
      ],
      "metadata": {
        "id": "nADSifVmSZ-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "# Model sederhana dengan Embedding layer\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=64, input_length=10),  # Embedding layer\n",
        "    Flatten(),  # Mengubah vektor 3D menjadi 2D\n",
        "    Dense(1, activation='sigmoid')  # Layer output\n",
        "])\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Dummy data\n",
        "import numpy as np\n",
        "x_train = np.random.randint(1000, size=(32, 10))  # 32 sampel, panjang sequence 10\n",
        "y_train = np.random.randint(2, size=(32, 1))  # Label biner\n",
        "\n",
        "# Melatih model\n",
        "model.fit(x_train, y_train, epochs=5)\n"
      ],
      "metadata": {
        "id": "5RZrFYMYSRSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan:\n",
        "- input_dim: Ukuran vocabulary.\n",
        "- output_dim: Dimensi vektor embedding.\n",
        "- input_length: Panjang sequence input."
      ],
      "metadata": {
        "id": "Ut9JB1KxSiwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Recurrent Neural Networks (RNNs)\n",
        "RNN digunakan untuk memproses data sequential, seperti teks. Dua varian populer adalah LSTM dan GRU."
      ],
      "metadata": {
        "id": "1_it670JSn09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "# Model RNN dengan LSTM\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=64, input_length=10),\n",
        "    LSTM(128),  # LSTM layer\n",
        "    Dense(1, activation='sigmoid')  # Layer output\n",
        "])\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Dummy data\n",
        "x_train = np.random.randint(1000, size=(32, 10))  # 32 sampel, panjang sequence 10\n",
        "y_train = np.random.randint(2, size=(32, 1))  # Label biner\n",
        "\n",
        "# Melatih model\n",
        "model.fit(x_train, y_train, epochs=5)\n"
      ],
      "metadata": {
        "id": "KxQcLwFgSgJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "# Model RNN dengan GRU\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=1000, output_dim=64, input_length=10),\n",
        "    GRU(128),  # GRU layer\n",
        "    Dense(1, activation='sigmoid')  # Layer output\n",
        "])\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Melatih model\n",
        "model.fit(x_train, y_train, epochs=5)\n"
      ],
      "metadata": {
        "id": "6lggGqwcSqcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan:\n",
        "- LSTM (Long Short-Term Memory):\n",
        "Memiliki mekanisme gating untuk mengatasi masalah vanishing gradient.\n",
        "Cocok untuk data sequence panjang.\n",
        "- GRU (Gated Recurrent Unit):\n",
        "Lebih ringan dibandingkan LSTM.\n",
        "Menggabungkan gate input dan forget menjadi satu."
      ],
      "metadata": {
        "id": "1btL-Ej6SwuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Transfer Learning\n",
        "\n",
        "Pre-trained Models: using pre-trained models (e.g., MobileNet, InceptionV3) for image classification tasks.\n",
        "Fine-tuning: fine-tuning pre-trained models to improve performance on specific datasets.\n",
        "Feature Extraction: using pre-trained models as feature extractors for new classification tasks."
      ],
      "metadata": {
        "id": "wZ344H4-S5eS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Pre-trained Models\n",
        "Pre-trained models adalah model yang telah dilatih pada dataset besar (seperti ImageNet) dan dapat digunakan untuk tugas image classification.\n",
        "\n",
        "Contoh dengan MobileNetV2:"
      ],
      "metadata": {
        "id": "9dmCUVyodr3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# Menggunakan model pre-trained MobileNetV2 tanpa top layer\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Membuat model dengan layer tambahan\n",
        "model = Sequential([\n",
        "    base_model,  # Pre-trained base\n",
        "    GlobalAveragePooling2D(),  # Pooling layer\n",
        "    Dense(128, activation='relu'),  # Fully connected layer\n",
        "    Dense(10, activation='softmax')  # Output layer untuk 10 kelas\n",
        "])\n",
        "\n",
        "# Membekukan layer pre-trained\n",
        "base_model.trainable = False\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Ringkasan model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_pCDFl4nSshD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Fine-tuning\n",
        "Fine-tuning adalah melatih ulang sebagian dari layer pre-trained model agar dapat lebih sesuai dengan dataset spesifik.\n",
        "\n",
        "Langkah-Langkah Fine-Tuning:\n",
        "Membekukan sebagian besar layer pre-trained.\n",
        "Membuka beberapa layer akhir untuk dilatih ulang pada dataset baru.\n",
        "Contoh Fine-Tuning:"
      ],
      "metadata": {
        "id": "Nz1fQlBXdy7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuka sebagian layer akhir untuk dilatih ulang\n",
        "base_model.trainable = True\n",
        "\n",
        "# Membekukan sebagian besar layer awal\n",
        "for layer in base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Kompilasi ulang model setelah membuka layer\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Melatih model (gunakan dataset Anda sendiri)\n",
        "# model.fit(train_dataset, validation_data=val_dataset, epochs=10)\n"
      ],
      "metadata": {
        "id": "QE241Wcsd0ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Feature Extraction\n",
        "Feature extraction menggunakan pre-trained model untuk mengekstrak fitur dari data baru tanpa melatih ulang model.\n",
        "\n",
        "Contoh Feature Extraction:"
      ],
      "metadata": {
        "id": "Q0pz8fDvd2hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Menggunakan MobileNetV2 untuk feature extraction\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Membekukan semua layer\n",
        "base_model.trainable = False\n",
        "\n",
        "# Memuat gambar contoh\n",
        "img_path = 'example_image.jpg'  # Ganti dengan path gambar Anda\n",
        "img = load_img(img_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = preprocess_input(img_array)\n",
        "\n",
        "# Mengekstrak fitur\n",
        "features = base_model.predict(img_array)\n",
        "print(\"Shape of extracted features:\", features.shape)\n"
      ],
      "metadata": {
        "id": "Mp3vvudtd6Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penjelasan Utama:\n",
        "Pre-trained Models:\n",
        "\n",
        "MobileNetV2, InceptionV3, ResNet50, dll., telah dilatih pada dataset ImageNet.\n",
        "Dapat digunakan langsung untuk klasifikasi atau sebagai dasar untuk tugas lain.\n",
        "Fine-tuning:\n",
        "\n",
        "Membuka sebagian layer agar dapat disesuaikan dengan dataset baru.\n",
        "Berguna jika dataset baru memiliki karakteristik yang berbeda dari dataset asli.\n",
        "Feature Extraction:\n",
        "\n",
        "Menggunakan model untuk menghasilkan representasi fitur dari data baru.\n",
        "Cocok untuk transfer learning dengan dataset kecil."
      ],
      "metadata": {
        "id": "8YgBa-8hd8lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Time Series and Forecasting\n",
        "a. Time Series Data Preparation\n",
        "Persiapan data melibatkan transformasi dataset menjadi format yang dapat digunakan untuk pelatihan model."
      ],
      "metadata": {
        "id": "Mlfm677yeD9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Fungsi untuk membuat data time series\n",
        "def create_time_series(data, window_size):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i+window_size])\n",
        "        y.append(data[i+window_size])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Contoh dataset\n",
        "data = np.sin(np.linspace(0, 100, 200))  # Sinyal sinusoidal\n",
        "window_size = 10\n",
        "X, y = create_time_series(data, window_size)\n",
        "\n",
        "# Bentuk data\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n"
      ],
      "metadata": {
        "id": "5yic_V34eI7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. RNNs for Time Series\n",
        "Menggunakan RNN (LSTM atau GRU) untuk memprediksi data time series."
      ],
      "metadata": {
        "id": "KsQoBYHaeQjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Membuat model LSTM\n",
        "model = Sequential([\n",
        "    LSTM(50, activation='relu', input_shape=(window_size, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Latih model\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))  # Reshape untuk LSTM\n",
        "model.fit(X, y, epochs=10, batch_size=16)\n"
      ],
      "metadata": {
        "id": "DrSeMWfceSAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Custom Training with tf.GradientTape\n",
        "a. Custom Training Loop\n",
        "Menggunakan tf.GradientTape untuk melatih model secara manual."
      ],
      "metadata": {
        "id": "2OhdgYKgeTyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat model sederhana\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Optimizer dan loss\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Custom training loop\n",
        "for epoch in range(10):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(X, training=True)\n",
        "        loss = loss_fn(y, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.numpy()}\")\n"
      ],
      "metadata": {
        "id": "p-uf5gf7egeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Custom Loss Functions\n",
        "Mendefinisikan fungsi loss kustom."
      ],
      "metadata": {
        "id": "edLULuN_eiwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))  # Mean Squared Error\n",
        "\n",
        "# Kompilasi ulang model dengan custom loss\n",
        "model.compile(optimizer='adam', loss=custom_loss)"
      ],
      "metadata": {
        "id": "e0O2Z7cEeiZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Model Training\n",
        "a. Loss Functions and Metrics\n",
        "Memahami berbagai loss functions dan metrics."
      ],
      "metadata": {
        "id": "RCycFQQlerBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh loss functions\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "metric = tf.keras.metrics.Accuracy()\n",
        "\n",
        "# Kompilasi model\n",
        "model.compile(optimizer='adam', loss=loss_fn, metrics=[metric])\n"
      ],
      "metadata": {
        "id": "bitvVW2neuaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Early Stopping and Checkpoints\n",
        "Menggunakan callback untuk menghentikan pelatihan lebih awal dan menyimpan model terbaik."
      ],
      "metadata": {
        "id": "I_JDBR0ReyA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create early stopping and checkpoints\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Model Checkpoint\n",
        "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Dummy data for demonstration (replace with your actual data)\n",
        "x_train = np.random.random((100, 10))\n",
        "y_train = np.random.randint(2, size=(100, 1))\n",
        "\n",
        "# Train the model with callbacks\n",
        "model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping, checkpoint])"
      ],
      "metadata": {
        "id": "pLpJAhn1ezxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Model Evaluation and Validation\n",
        "a. Confusion Matrix and Classification Report\n",
        "Menggunakan sklearn untuk evaluasi."
      ],
      "metadata": {
        "id": "JlB_4nlje2Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Prediksi\n",
        "y_pred = (model.predict(X) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(confusion_matrix(y, y_pred))\n",
        "print(classification_report(y, y_pred))\n"
      ],
      "metadata": {
        "id": "GfqVE8NhfXci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROC and AUC"
      ],
      "metadata": {
        "id": "ClZwZwhKfZ8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y, model.predict(X))\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot\n",
        "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FSSn91Evf3ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Deployment and Serving\n",
        "a. Model Saving and Loading\n",
        "Menyimpan dan memuat model."
      ],
      "metadata": {
        "id": "kdQ-ujsqgAYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model\n",
        "model.save('my_model.h5')\n",
        "\n",
        "# Muat model\n",
        "from tensorflow.keras.models import load_model\n",
        "loaded_model = load_model('my_model.h5')\n"
      ],
      "metadata": {
        "id": "0hqIbFyWf-Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. TensorFlow Lite\n",
        "Konversi model ke format TensorFlow Lite."
      ],
      "metadata": {
        "id": "1gwbI2z8gEkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Simpan model TFLite\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "0l1WevffgI_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Generative AI\n",
        "a. Basic Generative AI\n",
        "Contoh sederhana dengan GAN."
      ],
      "metadata": {
        "id": "UQJI0wQfgMSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Generator\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_dim=100),\n",
        "        layers.Dense(784, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Discriminator\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_dim=784),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n"
      ],
      "metadata": {
        "id": "x8osBA-VgOS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finish"
      ],
      "metadata": {
        "id": "qOc3_oEugQ_j"
      }
    }
  ]
}